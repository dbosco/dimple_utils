# LLM Configuration
# Maximum response tokens for LLM clients
# If not specified, defaults to 20480
llm.max_response_tokens=20480

# OpenAI specific configuration
openai.max.response_tokens=20480
openai.model=gpt-4o
openai.retry.delay=60
openai.max.retries=5

# Anthropic specific configuration
anthropic.max.response_tokens=20480
anthropic.model=claude-3-5-sonnet-20241022
anthropic.retry.delay=60
anthropic.max.retries=5

# Other LLM settings can be added here
# llm.temperature=0.0
# llm.retry_delay=60
# llm.max_retries=5
